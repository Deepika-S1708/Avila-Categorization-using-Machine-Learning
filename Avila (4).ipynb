{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\Krishna\\avila\\avila_combined.txt')\n",
    "df_train = pd.read_csv(r'D:\\Krishna\\avila\\avila-tr.txt')\n",
    "df_test = pd.read_csv(r'D:\\Krishna\\avila\\avila-ts.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "True in list(df.duplicated())\n",
    "\n",
    "print('train data')\n",
    "d = dict(df_train['class'].value_counts())\n",
    "\n",
    "keys = sorted(list(d.keys()))\n",
    "for key in keys:\n",
    "    print(key + ' ' + str(d[key]))\n",
    "    \n",
    "print('test data')\n",
    "d = dict(df_test['class'].value_counts())\n",
    "keys = sorted(list(d.keys()))\n",
    "for key in keys:\n",
    "    print(key + ' ' + str(d[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe()\n",
    "# df.head()\n",
    "# df = df.sample(frac = 1)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['intercolumnar distance', 'upper margin', 'lower margin',\n",
       "       'exploitation', 'row number', 'modular ratio', 'interlinear spacing',\n",
       "       'weight', 'peak number', 'modular ratio/ interlinear spacing',\n",
       "       ' class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop('class', axis=1)\n",
    "target = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x,train_y,test_y = df_train.drop('class', axis=1),df_test.drop('class', axis=1),df_train['class'],df_test['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_model = LogisticRegression(max_iter = 500, solver = 'lbfgs', multi_class = 'multinomial')\n",
    "logistic_model.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy using logistic regression:  0.5638542665388303\n",
      "Test accuracy using logistic regression:  0.5613682092555332\n"
     ]
    }
   ],
   "source": [
    "pred_train_y = logistic_model.predict(train_x)\n",
    "\n",
    "# Calculate the accuracy as our performance metric\n",
    "accuracy = metrics.accuracy_score(train_y, pred_train_y)\n",
    "print(\"Train accuracy using logistic regression: \", accuracy)\n",
    "\n",
    "pred_test_y = logistic_model.predict(test_x)\n",
    "\n",
    "# Calculate the accuracy as our performance metric\n",
    "accuracy = metrics.accuracy_score(test_y, pred_test_y)\n",
    "print(\"Test accuracy using logistic regression: \", accuracy)\n",
    "\n",
    "# pred_val_y = logistic_model.predict(val_data_x)\n",
    "\n",
    "# # Calculate the accuracy as our performance metric\n",
    "# accuracy = metrics.accuracy_score(val_data_y, pred_val_y)\n",
    "# print(\"validation accuracy using logistic regression: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=42, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_model = SVC(kernel='rbf',random_state=42,gamma='auto')\n",
    "# running \n",
    "svc_model.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy using logistic regression:  0.7358581016299137\n",
      "Test accuracy using logistic regression:  0.722525629970298\n"
     ]
    }
   ],
   "source": [
    "pred_train_y = svc_model.predict(train_x)\n",
    "\n",
    "# Calculate the accuracy as our performance metric\n",
    "accuracy = metrics.accuracy_score(train_y, pred_train_y)\n",
    "print(\"Train accuracy using logistic regression: \", accuracy)\n",
    "\n",
    "pred_test_y = svc_model.predict(test_x)\n",
    "\n",
    "# Calculate the accuracy as our performance metric\n",
    "accuracy = metrics.accuracy_score(test_y, pred_test_y)\n",
    "print(\"Test accuracy using logistic regression: \", accuracy)\n",
    "\n",
    "# pred_val_y = svc_model.predict(val_data_x)\n",
    "\n",
    "# # Calculate the accuracy as our performance metric\n",
    "# accuracy = metrics.accuracy_score(val_data_y, pred_val_y)\n",
    "# print(\"validation accuracy using logistic regression: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "\n",
    "knn_model = KNN()\n",
    "knn_model.fit(train_x,train_y)\n",
    "model = knn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy using KNN:  0.8318312559923298\n",
      "Test accuracy using KNN:  0.7495448883778864\n"
     ]
    }
   ],
   "source": [
    "pred_train_y = model.predict(train_x)\n",
    "\n",
    "# Calculate the accuracy as our performance metric\n",
    "accuracy = metrics.accuracy_score(train_y, pred_train_y)\n",
    "print(\"Train accuracy using KNN: \", accuracy)\n",
    "\n",
    "pred_test_y = model.predict(test_x)\n",
    "\n",
    "# Calculate the accuracy as our performance metric\n",
    "accuracy = metrics.accuracy_score(test_y, pred_test_y)\n",
    "print(\"Test accuracy using KNN: \", accuracy)\n",
    "\n",
    "# pred_val_y = model.predict(val_data_x)\n",
    "\n",
    "# # Calculate the accuracy as our performance metric\n",
    "# accuracy = metrics.accuracy_score(val_data_y, pred_val_y)\n",
    "# print(\"validation accuracy using KNN: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9864903707962058\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier(criterion='entropy',random_state=42)\n",
    "dt_model.fit(train_x,train_y)\n",
    "model = dt_model\n",
    "model_name = 'Decision Tree'\n",
    "\n",
    "pred_test_y = model.predict(test_x)\n",
    "\n",
    "accuracy = metrics.accuracy_score(train_y, pred_train_y)\n",
    "print(\"Train accuracy: \", accuracy)\n",
    "\n",
    "\n",
    "# Calculate the accuracy as our performance metric\n",
    "accuracy = metrics.accuracy_score(test_y, pred_test_y)\n",
    "print(\"Test accuracy: \", accuracy)\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix,classification_report\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# pred_test_y = model.predict(test_x)\n",
    "\n",
    "# labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'W', 'X', 'Y']\n",
    "# cm = confusion_matrix(test_y, pred_test_y, labels)\n",
    "# print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.993101465938488\n",
      "[[4274    0    0    0    1    4    1    3    2    0    1    0]\n",
      " [   0    5    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0  102    0    0    0    0    1    0    0    0    0]\n",
      " [   0    0    0  351    2    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0 1083    0    0    0    0    4    8    0]\n",
      " [  13    0    0    0    0 1943    1    5    0    0    0    0]\n",
      " [   0    0    0    0    0    0  447    0    0    0    0    0]\n",
      " [   3    0    2    0    2    2    0  511    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  832    0    0    0]\n",
      " [   0    0    0    0    3    0    0    0    0   42    0    0]\n",
      " [   0    0    0    0    6    0    0    0    0    0  515    1]\n",
      " [   0    0    0    0    0    0    0    0    0    0    7  260]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier(criterion='entropy',random_state=42,class_weight = 'balanced')\n",
    "dt_model.fit(train_x,train_y)\n",
    "model = dt_model\n",
    "model_name = 'Decision Tree'\n",
    "\n",
    "accuracy = metrics.accuracy_score(train_y, pred_train_y)\n",
    "print(\"Train accuracy: \", accuracy)\n",
    "\n",
    "\n",
    "pred_test_y = model.predict(test_x)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the accuracy as our performance metric\n",
    "accuracy = metrics.accuracy_score(test_y, pred_test_y)\n",
    "print(\"Test accuracy: \", accuracy)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pred_test_y = model.predict(test_x)\n",
    "\n",
    "labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'W', 'X', 'Y']\n",
    "cm = confusion_matrix(test_y, pred_test_y, labels)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                       random_state=7, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=7, n_estimators=100, class_weight = 'balanced')\n",
    "model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9842866724154451\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=7, n_estimators=100)\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "pred_train_y = model.predict(train_x)\n",
    "\n",
    "# Calculate the accuracy as our performance metric\n",
    "accuracy = metrics.accuracy_score(train_y, pred_train_y)\n",
    "print(\"Train accuracy: \", accuracy)\n",
    "pred_test_y = model.predict(test_x)\n",
    "\n",
    "# Calculate the accuracy as our performance metric\n",
    "accuracy = metrics.accuracy_score(test_y, pred_test_y)\n",
    "print(\"Test accuracy: \", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.9936763437769474\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=7, n_estimators=100, class_weight = 'balanced')\n",
    "model.fit(train_x, train_y)\n",
    "pred_train_y = model.predict(train_x)\n",
    "\n",
    "# Calculate the accuracy as our performance metric\n",
    "accuracy = metrics.accuracy_score(train_y, pred_train_y)\n",
    "print(\"Train accuracy: \", accuracy)\n",
    "pred_test_y = model.predict(test_x)\n",
    "\n",
    "# Calculate the accuracy as our performance metric\n",
    "accuracy = metrics.accuracy_score(test_y, pred_test_y)\n",
    "print(\"Test accuracy: \", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.993101465938488\n",
      "[[4274    0    0    0    1    4    1    3    2    0    1    0]\n",
      " [   0    5    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0  102    0    0    0    0    1    0    0    0    0]\n",
      " [   0    0    0  351    2    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0 1083    0    0    0    0    4    8    0]\n",
      " [  13    0    0    0    0 1943    1    5    0    0    0    0]\n",
      " [   0    0    0    0    0    0  447    0    0    0    0    0]\n",
      " [   3    0    2    0    2    2    0  511    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  832    0    0    0]\n",
      " [   0    0    0    0    3    0    0    0    0   42    0    0]\n",
      " [   0    0    0    0    6    0    0    0    0    0  515    1]\n",
      " [   0    0    0    0    0    0    0    0    0    0    7  260]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pred_test_y = model.predict(test_x)\n",
    "\n",
    "labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'W', 'X', 'Y']\n",
    "cm = confusion_matrix(test_y, pred_test_y, labels)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      1.00      1.00      4286\n",
      "           B       1.00      1.00      1.00         5\n",
      "           C       0.98      0.99      0.99       103\n",
      "           D       1.00      0.99      1.00       353\n",
      "           E       0.99      0.99      0.99      1095\n",
      "           F       1.00      0.99      0.99      1962\n",
      "           G       1.00      1.00      1.00       447\n",
      "           H       0.98      0.98      0.98       520\n",
      "           I       1.00      1.00      1.00       832\n",
      "           W       0.91      0.93      0.92        45\n",
      "           X       0.97      0.99      0.98       522\n",
      "           Y       1.00      0.97      0.98       267\n",
      "\n",
      "    accuracy                           0.99     10437\n",
      "   macro avg       0.98      0.99      0.99     10437\n",
      "weighted avg       0.99      0.99      0.99     10437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, pred_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10430"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy:  0.984857197623155\n"
     ]
    }
   ],
   "source": [
    "pred_val_y = model.predict(val_data_x)\n",
    "\n",
    "# Calculate the accuracy as our performance metric\n",
    "accuracy = metrics.accuracy_score(val_data_y, pred_val_y)\n",
    "print(\"validation accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'W', 'X', 'Y']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(set(val_data_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2100    0    0    0    0    1    0    0    0    0    0    0]\n",
      " [   0    3    0    0    0    0    0    0    0    0    0    0]\n",
      " [   1    0   50    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0  167    3    0    0    0    0    0    0    0]\n",
      " [   2    0    0    0  564    0    0    0    0    0    0    0]\n",
      " [   8    0    0    0    2 1010    1    2    0    0    0    0]\n",
      " [   3    0    0    0    0    0  227    0    0    0    0    0]\n",
      " [   9    0    1    0    2    2    0  248    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  422    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   25    0    0]\n",
      " [   1    0    0    0    5    0    0    0    0    0  238    0]\n",
      " [   1    0    0    0    1    0    0    0    0    0    0  118]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'W', 'X', 'Y']\n",
    "cm = confusion_matrix(val_data_y, pred_val_y, labels)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.99      1.00      0.99      2101\n",
      "           B       1.00      1.00      1.00         3\n",
      "           C       0.98      0.98      0.98        51\n",
      "           D       1.00      0.98      0.99       170\n",
      "           E       0.98      1.00      0.99       566\n",
      "           F       1.00      0.99      0.99      1023\n",
      "           G       1.00      0.99      0.99       230\n",
      "           H       0.99      0.95      0.97       262\n",
      "           I       1.00      1.00      1.00       422\n",
      "           W       1.00      1.00      1.00        25\n",
      "           X       1.00      0.98      0.99       244\n",
      "           Y       1.00      0.98      0.99       120\n",
      "\n",
      "    accuracy                           0.99      5217\n",
      "   macro avg       0.99      0.99      0.99      5217\n",
      "weighted avg       0.99      0.99      0.99      5217\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_data_y, pred_val_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
